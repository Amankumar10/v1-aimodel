{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff75bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aman\\Desktop\\Compatika-v1\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "DATA_PATH = \"v1trainingdataset40mb.txt\"   # your input txt file (USER/COMPATIKA pairs)\n",
    "OUTPUT_DIR = \"compatika_v1_lstm_checkpoints\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d28d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG ==========\n",
    "DATA_PATH = \"v1trainingdataset40mb.txt\"   # your input txt file (USER/COMPATIKA pairs)\n",
    "OUTPUT_DIR = \"compatika_v1_lstm_checkpoints\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935429b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer\n",
    "VOCAB_SIZE = 10000          # choose 8000-12000 as desired\n",
    "MIN_FREQ = 2\n",
    "TOKENIZER_FILE = os.path.join(OUTPUT_DIR, \"bpe_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d2c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset / length\n",
    "MAX_INPUT_LEN = 160         # context length for prompt (128-256)\n",
    "MAX_TARGET_LEN = 128        # max reply length\n",
    "BLOCK_INPUT = MAX_INPUT_LEN\n",
    "BLOCK_TARGET = MAX_TARGET_LEN\n",
    "\n",
    "# Model size => aim for 5-10M params (adjust to fit)\n",
    "EMBED_DIM = 256             # token embedding dim\n",
    "HIDDEN_SIZE = 256        # LSTM hidden size\n",
    "NUM_LAYERS = 2              # encoder & decoder layers\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 64             # reduce if OOM (try 32/16)\n",
    "EPOCHS = 6\n",
    "LR = 3e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Misc\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "BOS_TOKEN = \"[BOS]\"\n",
    "EOS_TOKEN = \"[EOS]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eb03ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 422648 prompt-reply pairs.\n"
     ]
    }
   ],
   "source": [
    "# ========== HELPERS: read prompt/reply from file ==========\n",
    "def read_pairs_from_txt(path: str) -> List[Tuple[str,str]]:\n",
    "    raw = Path(path).read_text(encoding=\"utf-8\")\n",
    "    blocks = re.split(r'\\n\\s*\\n', raw.strip())\n",
    "    pairs = []\n",
    "    for b in blocks:\n",
    "        # match USER: ... and COMPATIKA: ...\n",
    "        user_m = re.search(r'USER:\\s*(.+?)(?:\\n|$)', b, flags=re.IGNORECASE)\n",
    "        comp_m = re.search(r'COMPATIKA:\\s*(.+)', b, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if user_m and comp_m:\n",
    "            user = user_m.group(1).strip()\n",
    "            comp = comp_m.group(1).strip()\n",
    "            if user and comp:\n",
    "                prompt = f\"USER: {user}\\nCOMPATIKA:\"\n",
    "                reply = comp\n",
    "                pairs.append((prompt, reply))\n",
    "    return pairs\n",
    "\n",
    "if not Path(DATA_PATH).exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\")\n",
    "\n",
    "pairs = read_pairs_from_txt(DATA_PATH)\n",
    "print(f\"Loaded {len(pairs)} prompt-reply pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b659c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from compatika_v1_lstm_checkpoints\\bpe_tokenizer.json\n",
      "Vocab size: 10000 pad: 0 unk: 1\n"
     ]
    }
   ],
   "source": [
    "# ========== TRAIN OR LOAD TOKENIZER ==========\n",
    "if not os.path.exists(TOKENIZER_FILE):\n",
    "    print(\"Training BPE tokenizer...\")\n",
    "    tok = Tokenizer(models.BPE(unk_token=UNK_TOKEN))\n",
    "    tok.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, min_frequency=MIN_FREQ,\n",
    "                                  special_tokens=[PAD_TOKEN, UNK_TOKEN, BOS_TOKEN, EOS_TOKEN])\n",
    "    tok.train([DATA_PATH], trainer)\n",
    "    tok.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "    tok.enable_truncation(max_length=MAX_INPUT_LEN + MAX_TARGET_LEN)  # safe limit\n",
    "    tok.save(TOKENIZER_FILE)\n",
    "    print(\"Tokenizer saved ->\", TOKENIZER_FILE)\n",
    "else:\n",
    "    print(\"Loading tokenizer from\", TOKENIZER_FILE)\n",
    "    tok = Tokenizer.from_file(TOKENIZER_FILE)\n",
    "\n",
    "vocab_size = tok.get_vocab_size()\n",
    "pad_id = tok.token_to_id(PAD_TOKEN)\n",
    "unk_id = tok.token_to_id(UNK_TOKEN)\n",
    "bos_id = tok.token_to_id(BOS_TOKEN)\n",
    "eos_id = tok.token_to_id(EOS_TOKEN)\n",
    "print(\"Vocab size:\", vocab_size, \"pad:\", pad_id, \"unk:\", unk_id)\n",
    "\n",
    "def encode(text: str) -> List[int]:\n",
    "    return tok.encode(text).ids\n",
    "\n",
    "def decode(ids: List[int]) -> str:\n",
    "    return tok.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed09887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataset with 422648 examples\n"
     ]
    }
   ],
   "source": [
    "# ========== DATASET ==========\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_input, max_target, pad_id, bos_id, eos_id):\n",
    "        self.examples = []\n",
    "        for prompt, reply in pairs:\n",
    "            p_ids = tokenizer.encode(prompt).ids\n",
    "            r_ids = tokenizer.encode(\" \" + reply).ids  # leading space helps BPE\n",
    "            # ensure we have EOS for target\n",
    "            if len(r_ids) == 0:\n",
    "                continue\n",
    "            if r_ids[-1] != eos_id:\n",
    "                r_ids = r_ids + [eos_id]\n",
    "            # truncate if necessary\n",
    "            if len(p_ids) > max_input:\n",
    "                p_ids = p_ids[-max_input:]  # keep most recent tokens if too long\n",
    "            if len(r_ids) > max_target - 1:\n",
    "                r_ids = r_ids[:max_target - 1] + [eos_id]\n",
    "            self.examples.append((p_ids, r_ids))\n",
    "        if len(self.examples) == 0:\n",
    "            raise ValueError(\"No examples after tokenization/truncation.\")\n",
    "    def __len__(self): return len(self.examples)\n",
    "    def __getitem__(self, idx):\n",
    "        p_ids, r_ids = self.examples[idx]\n",
    "        # build decoder input: BOS + r_ids[:-1]\n",
    "        dec_in = [bos_id] + r_ids[:-1]\n",
    "        # pad sequences to fixed lengths\n",
    "        enc = p_ids[:] + [pad_id] * (MAX_INPUT_LEN - len(p_ids))\n",
    "        dec_in_padded = dec_in + [pad_id] * (MAX_TARGET_LEN - len(dec_in))\n",
    "        dec_out = r_ids + [pad_id] * (MAX_TARGET_LEN - len(r_ids))\n",
    "        # convert to tensors\n",
    "        return (torch.tensor(enc, dtype=torch.long),\n",
    "                torch.tensor(dec_in_padded, dtype=torch.long),\n",
    "                torch.tensor(dec_out, dtype=torch.long))\n",
    "\n",
    "dataset = Seq2SeqDataset(pairs, tok, MAX_INPUT_LEN, MAX_TARGET_LEN, pad_id, bos_id, eos_id)\n",
    "print(\"Prepared dataset with\", len(dataset),\"examples\")\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "338c0c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 4665344\n"
     ]
    }
   ],
   "source": [
    "# ========== MODEL: Encoder-Decoder LSTM with tied embeddings ==========\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, pad_id):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers,\n",
    "                               batch_first=True, bidirectional=False)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_size, num_layers=num_layers,\n",
    "                               batch_first=True, bidirectional=False)\n",
    "        # output projection uses tied embedding weights to save parameters\n",
    "        self.output_proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        # tie weights\n",
    "        self.output_proj.weight = self.embed.weight\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    def forward(self, enc_input, dec_input, enc_lengths=None):\n",
    "        # enc_input: B x L_enc\n",
    "        # dec_input: B x L_dec\n",
    "        emb_enc = self.embed(enc_input)   # B, L_enc, E\n",
    "        # optionally pack padded sequence if you track lengths (we used fixed length)\n",
    "        enc_outputs, (h_n, c_n) = self.encoder(emb_enc)  # h_n: num_layers x B x H\n",
    "        emb_dec = self.embed(dec_input)   # B, L_dec, E\n",
    "        dec_outputs, _ = self.decoder(emb_dec, (h_n, c_n))\n",
    "        logits = self.output_proj(dec_outputs)  # B, L_dec, V\n",
    "        return logits\n",
    "\n",
    "model = LSTMSeq2Seq(vocab_size, EMBED_DIM, HIDDEN_SIZE, NUM_LAYERS, pad_id).to(DEVICE)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Model parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d3c1379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:  30%|███       | 2001/6603 [02:08<05:04, 15.13it/s, loss=1.86]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_2000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:  61%|██████    | 4003/6603 [04:17<02:52, 15.08it/s, loss=1.68]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_4000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6:  91%|█████████ | 6001/6603 [06:21<00:38, 15.81it/s, loss=1.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_6000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|██████████| 6603/6603 [06:58<00:00, 15.77it/s, loss=1.61]\n",
      "Epoch 2/6:  21%|██        | 1398/6603 [01:25<06:30, 13.33it/s, loss=1.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_8000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6:  51%|█████▏    | 3400/6603 [03:27<03:39, 14.62it/s, loss=1.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_10000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6:  82%|████████▏ | 5398/6603 [05:34<01:18, 15.44it/s, loss=1.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_12000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|██████████| 6603/6603 [06:51<00:00, 16.06it/s, loss=1.52]\n",
      "Epoch 3/6:  12%|█▏        | 796/6603 [00:49<06:12, 15.59it/s, loss=1.52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_14000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6:  42%|████▏     | 2796/6603 [02:54<03:57, 16.05it/s, loss=1.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_16000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6:  73%|███████▎  | 4796/6603 [04:53<01:53, 15.99it/s, loss=1.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_18000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|██████████| 6603/6603 [06:46<00:00, 16.23it/s, loss=1.47]\n",
      "Epoch 4/6:   3%|▎         | 194/6603 [00:12<06:52, 15.55it/s, loss=1.46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_20000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6:  33%|███▎      | 2192/6603 [02:19<04:52, 15.09it/s, loss=1.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_22000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6:  63%|██████▎   | 4192/6603 [04:25<02:38, 15.20it/s, loss=1.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_24000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6:  94%|█████████▍| 6192/6603 [06:32<00:27, 15.20it/s, loss=1.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_26000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|██████████| 6603/6603 [06:58<00:00, 15.79it/s, loss=1.49]\n",
      "Epoch 5/6:  24%|██▍       | 1590/6603 [01:40<05:28, 15.28it/s, loss=1.4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_28000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6:  54%|█████▍    | 3590/6603 [03:46<03:15, 15.45it/s, loss=1.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_30000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6:  85%|████████▍ | 5590/6603 [05:50<01:01, 16.36it/s, loss=1.46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_32000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|██████████| 6603/6603 [06:51<00:00, 16.04it/s, loss=1.39]\n",
      "Epoch 6/6:  15%|█▍        | 988/6603 [00:59<05:47, 16.14it/s, loss=1.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_34000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6:  45%|████▌     | 2988/6603 [03:00<03:43, 16.19it/s, loss=1.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_36000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6:  76%|███████▌  | 4988/6603 [05:01<01:40, 16.06it/s, loss=1.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: compatika_v1_lstm_checkpoints\\step_38000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|██████████| 6603/6603 [06:38<00:00, 16.58it/s, loss=1.39]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Quick sanity: expected near 5-10M\n",
    "# If too big/small, adjust EMBED_DIM, HIDDEN_SIZE, NUM_LAYERS, or VOCAB_SIZE.\n",
    "\n",
    "# ========== TRAIN LOOP ==========\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "model.train()\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    running_loss = 0.0\n",
    "    for enc_in, dec_in, dec_out in pbar:\n",
    "        enc_in = enc_in.to(DEVICE)\n",
    "        dec_in = dec_in.to(DEVICE)\n",
    "        dec_out = dec_out.to(DEVICE)\n",
    "        logits = model(enc_in, dec_in)  # B, L_dec, V\n",
    "        B, L, V = logits.shape\n",
    "        loss = criterion(logits.view(B*L, V), dec_out.view(B*L))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            pbar.set_postfix({\"loss\": running_loss / 100})\n",
    "            running_loss = 0.0\n",
    "        if global_step % 2000 == 0:\n",
    "            ck = os.path.join(OUTPUT_DIR, f\"step_{global_step}.pt\")\n",
    "            torch.save({\"model\": model.state_dict(), \"tokenizer\": TOKENIZER_FILE}, ck)\n",
    "            print(\"Saved checkpoint:\", ck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e9e6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Saved final checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# final save\n",
    "torch.save({\"model\": model.state_dict(), \"tokenizer\": TOKENIZER_FILE}, os.path.join(OUTPUT_DIR, \"final.pt\"))\n",
    "print(\"Training finished. Saved final checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a84ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generation (likely gibberish before training):\n",
      "ĠI Ġhear Ġwhat Ġyou âĢĻ re Ġsaying .\n"
     ]
    }
   ],
   "source": [
    "# ========== GENERATION: greedy decoding ==========\n",
    "model.eval()\n",
    "@torch.no_grad()\n",
    "def generate_reply(user_text: str, max_len: int = 120):\n",
    "    prompt = f\"USER: {user_text}\\nCOMPATIKA:\"\n",
    "    enc_ids = tok.encode(prompt).ids\n",
    "    if len(enc_ids) > MAX_INPUT_LEN:\n",
    "        enc_ids = enc_ids[-MAX_INPUT_LEN:]\n",
    "    enc_tensor = torch.tensor(enc_ids + [pad_id]*(MAX_INPUT_LEN - len(enc_ids)), dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "    # initialize decoder input with BOS\n",
    "    dec_input = [bos_id] + [pad_id]*(MAX_TARGET_LEN-1)\n",
    "    dec_t = torch.tensor(dec_input, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "    # run encoder to get hidden states\n",
    "    with torch.no_grad():\n",
    "        emb_enc = model.embed(enc_tensor)\n",
    "        enc_out, (h_n, c_n) = model.encoder(emb_enc)\n",
    "        # decode step by step (greedy)\n",
    "        generated = []\n",
    "        h, c = h_n, c_n\n",
    "        prev_token = torch.tensor([[bos_id]], dtype=torch.long).to(DEVICE)\n",
    "        for step in range(max_len):\n",
    "            emb = model.embed(prev_token)  # 1,1,E\n",
    "            out, (h, c) = model.decoder(emb, (h, c))\n",
    "            logits = model.output_proj(out[:, -1, :])  # 1,V\n",
    "            next_id = torch.argmax(logits, dim=-1).item()\n",
    "            if next_id == eos_id or next_id == pad_id:\n",
    "                break\n",
    "            generated.append(next_id)\n",
    "            prev_token = torch.tensor([[next_id]], dtype=torch.long).to(DEVICE)\n",
    "        return tok.decode(generated)\n",
    "\n",
    "# quick test\n",
    "print(\"Sample generation (likely gibberish before training):\")\n",
    "print(generate_reply(\"I feel nervous about tomorrow.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a442c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- ------------\n",
      "anyio             4.12.0\n",
      "asttokens         3.0.1\n",
      "certifi           2025.11.12\n",
      "click             8.3.1\n",
      "colorama          0.4.6\n",
      "comm              0.2.3\n",
      "debugpy           1.8.17\n",
      "decorator         5.2.1\n",
      "einops            0.8.1\n",
      "exceptiongroup    1.3.1\n",
      "executing         2.2.1\n",
      "filelock          3.20.0\n",
      "fsspec            2025.12.0\n",
      "h11               0.16.0\n",
      "hf-xet            1.2.0\n",
      "httpcore          1.0.9\n",
      "httpx             0.28.1\n",
      "huggingface_hub   1.2.1\n",
      "idna              3.11\n",
      "ipykernel         7.1.0\n",
      "ipython           8.37.0\n",
      "jedi              0.19.2\n",
      "Jinja2            3.1.6\n",
      "jupyter_client    8.6.3\n",
      "jupyter_core      5.9.1\n",
      "MarkupSafe        2.1.5\n",
      "matplotlib-inline 0.2.1\n",
      "mpmath            1.3.0\n",
      "nest-asyncio      1.6.0\n",
      "networkx          3.3\n",
      "numpy             2.1.2\n",
      "packaging         25.0\n",
      "parso             0.8.5\n",
      "pillow            11.3.0\n",
      "pip               21.2.3\n",
      "platformdirs      4.5.1\n",
      "prompt_toolkit    3.0.52\n",
      "psutil            7.1.3\n",
      "pure_eval         0.2.3\n",
      "Pygments          2.19.2\n",
      "python-dateutil   2.9.0.post0\n",
      "PyYAML            6.0.3\n",
      "pyzmq             27.1.0\n",
      "setuptools        57.4.0\n",
      "shellingham       1.5.4\n",
      "six               1.17.0\n",
      "stack-data        0.6.3\n",
      "sympy             1.13.1\n",
      "tokenizers        0.22.1\n",
      "torch             2.5.1+cu121\n",
      "torchvision       0.20.1+cu121\n",
      "tornado           6.5.2\n",
      "tqdm              4.67.1\n",
      "traitlets         5.14.3\n",
      "typer-slim        0.20.0\n",
      "typing_extensions 4.15.0\n",
      "wcwidth           0.2.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\aman\\Desktop\\Compatika-v1\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8556ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.5.1+cu121\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\aman\\desktop\\compatika-v1\\venv\\lib\\site-packages\n",
      "Requires: typing-extensions, filelock, jinja2, fsspec, sympy, networkx\n",
      "Required-by: torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96bd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
